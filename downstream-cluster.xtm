
(sys:load "libs/core/math.xtm")
(sys:load "libs/external/nanomsg.xtm")
(sys:load "mpi.xtm")

(bind-val UPSTREAM_SOCKET i32 -1)
(bind-val DOWNSTREAM_SOCKET i32 -1)
(bind-val NUMBER_OF_DOWNSTREAM_HOSTS i32 2)

;; try and connect to the next cluster
;; DOWNSTREAM_HOSTS and
;; HAS_DOWNSTREAM will be set in the eval call to the process
(bind-func bind-downstream
  (lambda ()
    (set! DOWNSTREAM_SOCKET (nn_socket AF_SP NN_PUSH))
    (if (< DOWNSTREAM_SOCKET 0)
        (println "Socket generation failed"))
    (nn_bind DOWNSTREAM_SOCKET "tcp://*:9000")))

;; this is what the upstream will connect to 
(bind-func connect-upstream
  (lambda ()
    (let ((address:i8* (zalloc 100)))
      (set! UPSTREAM_SOCKET (nn_socket AF_SP NN_PULL))
      (if (< UPSTREAM_SOCKET 0)
          (println "Upstream socket generation failed"))
      (nn_connect UPSTREAM_SOCKET UPSTREAM_HOST1)
      (nn_connect UPSTREAM_SOCKET UPSTREAM_HOST2))))

(bind-func send-downstream
  (lambda (buf:float*)
    (nn_send DOWNSTREAM_SOCKET (cast buf i8*) 4 NN_DONTWAIT)))

;; do something to the received values, 
(bind-func compute:[float,float*]*
  (lambda (buffer_ptr)
    (let ((val (pref buffer_ptr 0)))
      ;; this is just some random stuff, change as required
      (set! val (if (< val 0.) (* val 2.) (* val 3.))))))

;; for any final computations before it either gets sent downstream or is discarded
(bind-func root_node_compute
  (lambda (mpi_buffer:float*)
    1))

(bind-func main
  (lambda ()
    (MPI_Init null null)
    (let ((world_size_ptr:i32* (zalloc))
          (world_size (begin
                        (MPI_Comm_size MPI_COMM_WORLD world_size_ptr)
                        (pref world_size_ptr 0)))
          (world_rank_ptr:i32* (zalloc))
          (world_rank (begin
                        (MPI_Comm_rank MPI_COMM_WORLD world_rank_ptr)
                        (pref world_rank_ptr 0)))
          (mpi_buffer:float* (zalloc))
          (nanomsg_receive_buffer:float* (zalloc)))
      
      (if (= world_rank 0)
          (begin 
            (connect-upstream)
            (bind-downstream)
            (while #t
              ;; round robin data between the cluster computer nodes
              (doloop (i world_size)
                (if (= i 0) ;; don't send to itself
                    1 
                    (begin 
                      ;; get data from upstream
                      (nn_recv UPSTREAM_SOCKET nanomsg_receive_buffer 4 0)
                      ;; give to node i
                      (MPI_Send (cast nanomsg_receive_buffer i8*) 1 MPI_FLOAT i 0 MPI_COMM_WORLD)
                      1)))

              (doloop (i world_size)
                (if (= i 0)
                    1
                    (begin
                      ;; receive data from node i
                      (MPI_Recv (cast mpi_buffer i8*) 1 MPI_FLOAT MPI_ANY_SOURCE 0 MPI_COMM_WORLD MPI_STATUS_IGNORE)
                      ;; do something to it, or not
                      (root_node_compute mpi_buffer)
                      ;; send-downstream (this doesn't block)
                      (send-downstream mpi_buffer)
                      )))))
          (begin 
            (while #t
              ;; get the new thing
              (MPI_Recv (cast mpi_buffer i8*) 1 MPI_FLOAT 0 0 MPI_COMM_WORLD MPI_STATUS_IGNORE)
              (compute mpi_buffer)
              (MPI_Send (cast mpi_buffer i8*) 1 MPI_FLOAT 0 0 MPI_COMM_WORLD)))))))
